#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Cockswain Task Importer v1.3
- å¾ /tasks/processed è®€ JSON
- å¯«é€² MySQL
- æˆåŠŸçš„æ¬åˆ° /tasks/archived
- é—œé–‰ MySQL SSLï¼Œé¿å…ç’°å¢ƒæ²’ ssl.wrap_socket çš„éŒ¯
"""

import os
import json
import shutil
import datetime
from pathlib import Path
import mysql.connector

BASE = Path("/srv/cockswain-core")
PROCESSED = BASE / "tasks" / "processed"
ARCHIVE = BASE / "tasks" / "archived"
LOG = BASE / "logs" / "import-tasks.log"
ENV_FILE = BASE / ".env"

ARCHIVE.mkdir(parents=True, exist_ok=True)
LOG.parent.mkdir(parents=True, exist_ok=True)

def log(msg: str):
    ts = datetime.datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
    line = f"{ts} {msg}"
    with open(LOG, "a", encoding="utf-8") as f:
        f.write(line + "\n")
    print(line)

def load_env(path: Path):
    env = {}
    if not path.exists():
        log(f"âš ï¸ æœªæ‰¾åˆ° {path}ï¼Œå°‡ä½¿ç”¨é è¨­é€£ç·šåƒæ•¸")
        return env
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith("#") or "=" not in line:
                continue
            k, v = line.split("=", 1)
            env[k.strip()] = v.strip()
    return env

def get_db_connection(env):
    host = env.get("MYSQL_HOST", "localhost")
    port = int(env.get("MYSQL_PORT", 3306))
    user = env.get("MYSQL_USER", "cockswain_core")
    password = env.get("MYSQL_PASSWORD", "")
    database = env.get("MYSQL_DATABASE", "cockswain")

    try:
        conn = mysql.connector.connect(
            host=host,
            port=port,
            user=user,
            password=password,
            database=database,
            ssl_disabled=True,  # ğŸ‘ˆ é—œæ‰ SSLï¼Œé¿å… ssl.wrap_socket éŒ¯
        )
        return conn
    except mysql.connector.Error as e:
        log(f"âŒ è³‡æ–™åº«é€£ç·šå¤±æ•—: {e}")
        return None

def import_task_file(conn, file_path: Path):
    # é€™è£¡å…ˆç”¨æœ€é€šç”¨çš„æ¬„ä½ï¼štitle + content
    # ä¹‹å¾Œä½ è¦æ”¹æˆ task_ingest çš„æ¬„ä½æˆ‘å€‘å†èª¿
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        title = data.get("title", file_path.stem)
        content = json.dumps(data, ensure_ascii=False)
        created_at = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        cur = conn.cursor()
        # å…ˆå¯«é€² tasksï¼Œè‹¥ä½ åªå»ºç«‹äº† task_ingest å°±æŠŠé€™æ®µæ›æ‰
        cur.execute("""
            INSERT INTO tasks (title, content, created_at)
            VALUES (%s, %s, %s)
        """, (title, content, created_at))
        conn.commit()
        cur.close()

        log(f"âœ… åŒ¯å…¥æˆåŠŸï¼š{file_path.name}")
        return True
    except Exception as e:
        log(f"âŒ åŒ¯å…¥å¤±æ•—ï¼š{file_path.name}ï¼ŒåŸå› ï¼š{e}")
        return False

def main():
    env = load_env(ENV_FILE)
    conn = get_db_connection(env)
    if not conn:
        log("âŒ ç„¡æ³•é€£ç·šè³‡æ–™åº«ï¼Œç¨‹åºçµæŸã€‚")
        return

    files = sorted(PROCESSED.glob("*.json"))
    if not files:
        log("â„¹ï¸ æ²’æœ‰ä»»å‹™æª”å¯åŒ¯å…¥ã€‚")
        conn.close()
        return

    log(f"ğŸ” æ‰¾åˆ° {len(files)} å€‹ä»»å‹™æª”ï¼Œé–‹å§‹åŒ¯å…¥...")
    for fp in files:
        ok = import_task_file(conn, fp)
        if ok:
            archived_path = ARCHIVE / fp.name
            shutil.move(str(fp), str(archived_path))
            log(f"ğŸ“¦ å·²å°å­˜ï¼š{archived_path}")

    conn.close()
    log("ğŸ¯ åŒ¯å…¥ä½œæ¥­å®Œæˆã€‚")

if __name__ == "__main__":
    main()
